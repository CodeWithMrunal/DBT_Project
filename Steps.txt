1. start kafka:
-> sudo systemctl start kafka

2. create kafka topics:
-> /usr/local/kafka/bin/kafka-topics.sh --create --topic tweets --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1;
/usr/local/kafka/bin/kafka-topics.sh --create --topic top_tweets --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1;
/usr/local/kafka/bin/kafka-topics.sh --create --topic hashtags --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1;
/usr/local/kafka/bin/kafka-topics.sh --create --topic top_hashtags --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1;

3. check if the topics are created:
-> /usr/local/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092

4. start kafka_producer.py:
-> python3 kafka_producer.py

5. check if all the topics are receiving data from the producer:
-> /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic top_hashtags --from-beginning

6. Run spark to consume data from the kafka topic:
-> /opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 ss_cons_lang.py

7. open the database and check the table : You should see the data getting inserted periodically after batches

8. As the stream is running run the batch processing:
-> python3 batch_analysis.py